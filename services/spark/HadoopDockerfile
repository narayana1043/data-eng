# --------------------------------------------------
# Ubuntu base
# --------------------------------------------------
FROM ubuntu:22.04

ENV DEBIAN_FRONTEND=noninteractive

# --------------------------------------------------
# Install OS dependencies
# --------------------------------------------------
RUN apt-get update && apt-get install -y \
    openjdk-11-jdk \
    python3 \
    python3-pip \
    curl \
    wget \
    net-tools \
    procps \
    sudo \
    vim \
    && rm -rf /var/lib/apt/lists/*

# --------------------------------------------------
# Hadoop user
# --------------------------------------------------
RUN useradd -ms /bin/bash hadoop && \
    echo "hadoop ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# --------------------------------------------------
# Hadoop installation
# --------------------------------------------------
ENV HADOOP_VERSION=3.3.6
ENV HADOOP_HOME=/opt/hadoop

COPY tmp/hadoop-${HADOOP_VERSION}.tar.gz /tmp/

RUN tar -xzf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /opt && \
    mv /opt/hadoop-${HADOOP_VERSION} ${HADOOP_HOME} && \
    rm /tmp/hadoop-${HADOOP_VERSION}.tar.gz

# --------------------------------------------------
# Environment
# --------------------------------------------------
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV YARN_CONF_DIR=$HADOOP_CONF_DIR
ENV PYSPARK_PYTHON=/usr/bin/python3
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3
ENV PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

# --------------------------------------------------
# HDFS directories
# --------------------------------------------------
RUN mkdir -p /data/hdfs/namenode \
             /data/hdfs/datanode && \
    chown -R hadoop:hadoop /data /opt/hadoop

# --------------------------------------------------
# Copy Hadoop configs
# --------------------------------------------------
COPY core-site.xml $HADOOP_CONF_DIR/
COPY hdfs-site.xml $HADOOP_CONF_DIR/
COPY yarn-site.xml $HADOOP_CONF_DIR/

RUN chown -R hadoop:hadoop $HADOOP_CONF_DIR

RUN rm -rf /tmp/*

USER hadoop
WORKDIR /home/hadoop

# --------------------------------------------------
# Startup: HDFS + YARN
# --------------------------------------------------
CMD bash -c "\
  set -e && \
  java -version && \
  python3 --version && \
  \
  if [ ! -d /data/hdfs/namenode/current ]; then \
    hdfs namenode -format -force; \
  fi && \
  \
  hdfs --daemon start namenode && \
  hdfs --daemon start datanode && \
  until hdfs dfs -ls / >/dev/null 2>&1; do sleep 2; done && \
  \
  yarn --daemon start resourcemanager && \
  yarn --daemon start nodemanager && \
  until yarn node -list >/dev/null 2>&1; do sleep 2; done && \
  \
  hdfs dfs -mkdir -p /tmp /user/spark /user/delta /spark/spark-logs && \
  hdfs dfs -chown -R spark:spark /tmp /user/spark /user/delta /spark/ && \
  hdfs dfs -chmod -R 755 /tmp /user/spark /user/delta /spark/ && \
  \
  hdfs dfs -mkdir -p /tmp/logs && \
  hdfs dfs -chmod -R 1777 /tmp/logs && \
  \
  hdfs dfs -mkdir -p /user/hive/warehouse && \
  hdfs dfs -chown -R spark:spark /user/hive && \
  \
  hdfs dfs -mkdir -p /user/spark/checkpoints/actors && \
  hdfs dfs -chown -R spark:spark /user/spark && \
  hdfs dfs -chmod -R 755 /user/spark && \
  \
  echo 'HDFS + YARN READY' && \
  tail -f /dev/null \
"
