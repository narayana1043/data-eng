{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccf52f8-a734-4064-87da-5c1d9d4bb0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/05 20:32:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"DeltaJupyter\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n",
    "\n",
    "spark.conf.set(\n",
    "  \"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4975dfb-118d-45cc-ba63-05b56b796e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Veera\", \"Engineering\"),\n",
    "    (2, \"Asha\", \"Finance\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"dept\"])\n",
    "\n",
    "df.write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"/opt/spark/work-dir/data/delta/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91dd7d3f-2994-473f-8c49-e238df6cb449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "| id| name|       dept|\n",
      "+---+-----+-----------+\n",
      "|  1|Veera|Engineering|\n",
      "|  2| Asha|    Finance|\n",
      "+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\") \\\n",
    "  .load(\"/opt/spark/work-dir/data/delta/employees\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5936964-d0ee-4149-9df3-8077d41dc1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json\n"
     ]
    }
   ],
   "source": [
    "ls data/delta/employees/_delta_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d267867-8537-4cc5-b72e-5b18dab8fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [(3, \"Raj\", \"Risk\")]\n",
    "spark.createDataFrame(new_data, [\"id\",\"name\",\"dept\"]) \\\n",
    "  .write.format(\"delta\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(\"/opt/spark/work-dir/data/delta/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f326f99-d080-4d4f-801c-4668f954c45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json  00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "ls data/delta/employees/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e2c769-f87b-4d61-ba3f-3fdb932e4337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-12-19 03:34:05.083|NULL  |NULL    |WRITE    |{mode -> Append, partitionBy -> []}   |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1399}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.1.0|\n",
      "|0      |2025-12-19 03:33:28.928|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 2428}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.1.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(\n",
    "    spark, \"/opt/spark/work-dir/data/delta/employees\"\n",
    ")\n",
    "\n",
    "delta_table.history().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38afa5be-b9f8-4a79-806b-94fb0e93414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "| id| name|       dept|\n",
      "+---+-----+-----------+\n",
      "|  1|Veera|Engineering|\n",
      "|  2| Asha|    Finance|\n",
      "+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", 0) \\\n",
    "  .load(\"/opt/spark/work-dir/data/delta/employees\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21fed61-bab0-4615-831f-a01cb62f1a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 463bfa90-7451-4fb5-b763-56023e373d58).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n-- dept: string (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n-- dept: string (nullable = true)\n-- salary: long (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m bad_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m      2\u001b[0m     [(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m999\u001b[39m)],\n\u001b[1;32m      3\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdept\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mbad_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/spark/work-dir/data/delta/employees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 463bfa90-7451-4fb5-b763-56023e373d58).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n-- dept: string (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n-- dept: string (nullable = true)\n-- salary: long (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "bad_df = spark.createDataFrame(\n",
    "    [(4, \"Bad\", \"Ops\", 999)],\n",
    "    [\"id\", \"name\", \"dept\", \"salary\"]\n",
    ")\n",
    "\n",
    "bad_df.write.format(\"delta\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(\"/opt/spark/work-dir/data/delta/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0143d804-2a21-4aaf-904e-dd497bb7179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "bad_df.write.format(\"delta\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(\"/opt/spark/work-dir/data/delta/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ddbaec-d0eb-4e50-8112-e1325bb548db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "| id| name|       dept|salary|\n",
      "+---+-----+-----------+------+\n",
      "|  4|  Bad|        Ops|   999|\n",
      "|  1|Veera|Engineering|  NULL|\n",
      "|  2| Asha|    Finance|  NULL|\n",
      "|  3|  Raj|       Risk|  NULL|\n",
      "+---+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\") \\\n",
    "  .load(\"/opt/spark/work-dir/data/delta/employees\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e560b8-1a6f-470e-9d4a-2a8c625ce206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 03:34:50 ERROR MergeIntoCommand: Fatal error in MERGE with materialized source in attempt 1.\n",
      "org.apache.spark.sql.delta.DeltaAnalysisException: [DELTA_SCHEMA_CHANGE_SINCE_ANALYSIS] The schema of your Delta table has changed in an incompatible way since your DataFrame\n",
      "or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\n",
      "Changes:\n",
      "Latest schema has additional field(s): salary\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.schemaChangedSinceAnalysis(DeltaErrors.scala:553)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.schemaChangedSinceAnalysis$(DeltaErrors.scala:540)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.schemaChangedSinceAnalysis(DeltaErrors.scala:3203)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:89)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:83)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:223)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:83)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:60)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:60)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:60)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:81)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:138)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:106)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:94)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:60)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:138)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:113)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:60)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$2(DeltaMergeBuilder.scala:294)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.delta.DeltaTableUtils$.withActiveSession(DeltaTable.scala:491)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:269)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:99)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:85)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:149)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:267)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_SCHEMA_CHANGE_SINCE_ANALYSIS] The schema of your Delta table has changed in an incompatible way since your DataFrame\nor DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\nChanges:\nLatest schema has additional field(s): salary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[1;32m      3\u001b[0m updates \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m      4\u001b[0m     [(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNina\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLegal\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      5\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdept\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdelta_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[43mupdates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt.id = s.id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhenMatchedUpdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms.dept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhenNotMatchedInsertAll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/delta/tables.py:1022\u001b[0m, in \u001b[0;36mDeltaMergeBuilder.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m0.4\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m    Execute the merge operation based on the built matched and not matched actions.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03m    See :py:class:`~delta.tables.DeltaMergeBuilder` for complete usage details.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1022\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DELTA_SCHEMA_CHANGE_SINCE_ANALYSIS] The schema of your Delta table has changed in an incompatible way since your DataFrame\nor DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\nChanges:\nLatest schema has additional field(s): salary"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "updates = spark.createDataFrame(\n",
    "    [(2, \"Asha\", \"HR\"), (5, \"Nina\", \"Legal\")],\n",
    "    [\"id\", \"name\", \"dept\"]\n",
    ")\n",
    "\n",
    "delta_table.alias(\"t\") \\\n",
    "  .merge(\n",
    "      updates.alias(\"s\"),\n",
    "      \"t.id = s.id\"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set={\"dept\": \"s.dept\"}) \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b14ddeb-d5d6-4f4d-a720-1ab5bc69919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(\n",
    "    spark,\n",
    "    \"/opt/spark/work-dir/data/delta/employees\"\n",
    ")\n",
    "\n",
    "updates = spark.createDataFrame(\n",
    "    [(2, \"Asha\", \"HR\"), (5, \"Nina\", \"Legal\")],\n",
    "    [\"id\", \"name\", \"dept\"]\n",
    ")\n",
    "\n",
    "delta_table.alias(\"t\") \\\n",
    "  .merge(\n",
    "      updates.alias(\"s\"),\n",
    "      \"t.id = s.id\"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set={\n",
    "      \"dept\": \"s.dept\",\n",
    "      \"name\": \"s.name\"\n",
    "  }) \\\n",
    "  .whenNotMatchedInsert(values={\n",
    "      \"id\": \"s.id\",\n",
    "      \"dept\": \"s.dept\",\n",
    "      \"name\": \"s.name\",\n",
    "      \"salary\": \"NULL\"\n",
    "  }) \\\n",
    "  .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9a442d2-be8e-458b-85ff-7c4c6c3209e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "| id| name|       dept|salary|\n",
      "+---+-----+-----------+------+\n",
      "|  1|Veera|Engineering|  NULL|\n",
      "|  2| Asha|         HR|  NULL|\n",
      "|  3|  Raj|       Risk|  NULL|\n",
      "|  4|  Bad|        Ops|   999|\n",
      "|  5| Nina|      Legal|  NULL|\n",
      "+---+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\") \\\n",
    "  .load(\"/opt/spark/work-dir/data/delta/employees\") \\\n",
    "  .orderBy(\"id\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0e03d53-a093-4cec-830f-31b20ed53ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "delta_table.delete(\"dept = 'Risk'\")\n",
    "delta_table.update(\n",
    "    condition=\"dept = 'HR'\",\n",
    "    set={\"dept\": \"'People'\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98579e68-ae45-47af-b97f-fb1179b81207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "| id| name|       dept|salary|\n",
      "+---+-----+-----------+------+\n",
      "|  1|Veera|Engineering|  NULL|\n",
      "|  2| Asha|     People|  NULL|\n",
      "|  4|  Bad|        Ops|   999|\n",
      "|  5| Nina|      Legal|  NULL|\n",
      "+---+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\") \\\n",
    "  .load(\"/opt/spark/work-dir/data/delta/employees\") \\\n",
    "  .orderBy(\"id\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24d5ed92-f89d-42a5-9f5c-2aa4ae74d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|operation|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|7      |UPDATE   |{numRemovedFiles -> 1, numRemovedBytes -> 1132, numCopiedRows -> 1, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1140, numDeletionVectorsUpdated -> 0, scanTimeMs -> 860, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1144, rewriteTimeMs -> 279}                                                                                                                                                                                                                                                                                                                          |\n",
      "|6      |DELETE   |{numRemovedFiles -> 1, numRemovedBytes -> 938, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1350, numDeletionVectorsUpdated -> 0, numDeletedRows -> 1, scanTimeMs -> 1095, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 254}                                                                                                                                                                                                                                                                                                                             |\n",
      "|5      |MERGE    |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1132, numTargetBytesRemoved -> 1136, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, executionTimeMs -> 1934, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1067, numTargetRowsUpdated -> 2, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 476}|\n",
      "|4      |MERGE    |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1136, numTargetBytesRemoved -> 1136, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, executionTimeMs -> 1839, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1171, numTargetRowsUpdated -> 2, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 402}|\n",
      "|3      |MERGE    |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1136, numTargetBytesRemoved -> 966, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 2756, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1785, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 457} |\n",
      "|2      |WRITE    |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1725}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|1      |WRITE    |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1399}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|0      |WRITE    |{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 2428}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+-------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forPath(\n",
    "    spark, \"/opt/spark/work-dir/data/delta/employees\"\n",
    ")\n",
    "\n",
    "dt.history().select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d40b3172-5396-4717-aad7-431f517a4d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "OPTIMIZE delta.`/opt/spark/work-dir/data/delta/employees`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16fd6182-4f52-43fb-8614-7ef9decf63dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 12 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "VACUUM delta.`/opt/spark/work-dir/data/delta/employees` RETAIN 0 HOURS\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0334287e-92d6-4546-bf7a-d9313c080d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|operation   |operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|10     |VACUUM END  |{numDeletedFiles -> 12, numVacuumedDirectories -> 1}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|9      |VACUUM START|{numFilesToDelete -> 12, sizeOfDataToDelete -> 10639}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|8      |OPTIMIZE    |{numRemovedFiles -> 3, numRemovedBytes -> 3331, p25FileSize -> 1271, numDeletionVectorsRemoved -> 0, minFileSize -> 1271, numAddedFiles -> 1, maxFileSize -> 1271, p75FileSize -> 1271, p50FileSize -> 1271, numAddedBytes -> 1271}                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|7      |UPDATE      |{numRemovedFiles -> 1, numRemovedBytes -> 1132, numCopiedRows -> 1, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1140, numDeletionVectorsUpdated -> 0, scanTimeMs -> 860, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1144, rewriteTimeMs -> 279}                                                                                                                                                                                                                                                                                                                          |\n",
      "|6      |DELETE      |{numRemovedFiles -> 1, numRemovedBytes -> 938, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1350, numDeletionVectorsUpdated -> 0, numDeletedRows -> 1, scanTimeMs -> 1095, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 254}                                                                                                                                                                                                                                                                                                                             |\n",
      "|5      |MERGE       |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1132, numTargetBytesRemoved -> 1136, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, executionTimeMs -> 1934, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1067, numTargetRowsUpdated -> 2, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 476}|\n",
      "|4      |MERGE       |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1136, numTargetBytesRemoved -> 1136, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, executionTimeMs -> 1839, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1171, numTargetRowsUpdated -> 2, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 402}|\n",
      "|3      |MERGE       |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1136, numTargetBytesRemoved -> 966, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 2756, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1785, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 457} |\n",
      "|2      |WRITE       |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1725}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|1      |WRITE       |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1399}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|0      |WRITE       |{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 2428}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+-------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.history().select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93eb2eec-b248-4ca8-b1c2-177d58078853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "| id| name|       dept|salary|\n",
      "+---+-----+-----------+------+\n",
      "|  4|  Bad|        Ops|   999|\n",
      "|  2| Asha|     People|  NULL|\n",
      "|  5| Nina|      Legal|  NULL|\n",
      "|  1|Veera|Engineering|  NULL|\n",
      "+---+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE employees_delta\n",
    "USING DELTA\n",
    "LOCATION '/opt/spark/work-dir/data/delta/employees'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM employees_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03a942-a909-4cb9-869c-57d535690095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
